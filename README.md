# Coding LLMs from scratch
# Coding Llama-2
Learnt how to train and fine-tune Llama 2 model from scratch.

Learnt about transformers architecture, different attention mechanisms (MHA, MQA and GQA), KV cache, RoPE, and Hugginface Trainer in detail.

Created and trained a LLaMA 2 model with 100M parameters from scratch using PyTorch to do code completion.

# Coding Llama-3

Learnt how to train and fine-tune Llama 3 model from scratch.

Goal is to code LLaMA 3 from scratch in PyTorch to create models with sizes 3B, 6B, 35B and 45B params.

## Acknowledgements 
Learnt from Prince Canuma

